<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
<meta charset="utf-8">
<title>Thoughts &#8211; Data Ocean</title>
<meta name="description" content="Describe this nonsense.">
<meta name="keywords" content="Jekyll, theme, themes, responsive, blog, modern">

<!-- Twitter Cards -->
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://localhost:4000/images/background.jpeg">

<meta name="twitter:title" content="Thoughts">
<meta name="twitter:description" content="Describe this nonsense.">
<meta name="twitter:creator" content="@sosterburg">

<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Thoughts">
<meta property="og:description" content="Describe this nonsense.">
<meta property="og:url" content="http://localhost:4000/">
<meta property="og:site_name" content="Data Ocean">





<link rel="canonical" href="http://localhost:4000/">
<link href="http://localhost:4000/feed.xml" type="application/atom+xml" rel="alternate" title="Data Ocean Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="http://localhost:4000/assets/css/main.css">
<!-- Webfonts -->
<link href="https://fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic" rel="stylesheet" type="text/css">

<meta http-equiv="cleartype" content="on">

<!-- Load Modernizr -->
<script src="http://localhost:4000/assets/js/vendor/modernizr-2.6.2.custom.min.js"></script>

<!-- mathjax config similar to math.stackexchange -->
<!-- <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$']],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
});
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="http://localhost:4000/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="http://localhost:4000/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="http://localhost:4000/images/apple-touch-icon-precomposed.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="http://localhost:4000/images/apple-touch-icon-72x72-precomposed.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="http://localhost:4000/images/apple-touch-icon-114x114-precomposed.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://localhost:4000/images/apple-touch-icon-144x144-precomposed.png">



</head>

<body id="post-index" class="feature">

<!--[if lt IE 9]><div class="upgrade"><strong><a href="http://whatbrowser.org/">Your browser is quite old!</strong> Why not upgrade to a different browser to better enjoy this site?</a></div><![endif]-->
<nav id="dl-menu" class="dl-menuwrapper" role="navigation">
	<button class="dl-trigger">Open Menu</button>
	<ul class="dl-menu">
		<li><a href="http://localhost:4000/">Home</a></li>
		<li>
			<a href="#">About</a>
			<ul class="dl-submenu">
				<li>
					<img src="http://localhost:4000/images/avatar.jpg" alt="Stephan Osterburg photo" class="author-photo">
					<h4>Stephan Osterburg</h4>
					<p>Computer graphics veteran with 30+ years experience on the quest to discover the world of data.</p>
				</li>
				<li><a href="http://localhost:4000/about/"><span class="btn btn-inverse">Learn More</span></a></li>
				<li>
					<a href="mailto:stephanosterburg@me.com"><i class="fa fa-fw fa-envelope"></i> Email</a>
				</li>
				<li>
					<a href="https://twitter.com/sosterburg"><i class="fa fa-fw fa-twitter"></i> Twitter</a>
				</li>
				
				
				<li>
					<a href="https://linkedin.com/in/https://www.linkedin.com/in/stephanosterburg"><i class="fa fa-fw fa-linkedin"></i> LinkedIn</a>
				</li>
				<li>
					<a href="https://github.com/https://github.com/osterburg"><i class="fa fa-fw fa-github"></i> GitHub</a>
				</li>
				
				
				
				
			</ul><!-- /.dl-submenu -->
		</li>
		<li>
			<a href="#">Posts</a>
			<ul class="dl-submenu">
				<li><a href="http://localhost:4000/posts/">All Posts</a></li>
				<li><a href="http://localhost:4000/tags/">All Tags</a></li>
			</ul>
		</li>
		
	</ul><!-- /.dl-menu -->
</nav><!-- /.dl-menuwrapper -->


<div class="entry-header">
  
  
    <div class="entry-image">
      <img src="http://localhost:4000/images/background.jpeg" alt="Thoughts">
    </div><!-- /.entry-image -->
  
  <div class="header-title">
    <div class="header-title-wrap">
      <h1>Data Ocean</h1>
      <h2>Thoughts</h2>
    </div><!-- /.header-title-wrap -->
  </div><!-- /.header-title -->
</div><!-- /.entry-header -->

<div id="main" role="main">
  
<article class="hentry">
  <header>
    
      <div class="entry-image-index">
        <a href="http://localhost:4000/deep_learning_art_images/" title="Deep Learning and Art Images"><img src="http://localhost:4000/images/GoogleArt_cropped.png" alt="Deep Learning and Art Images"></a>
      </div><!-- /.entry-image -->
    
    <div class="entry-meta">
      <span class="entry-date date published updated"><time datetime="2019-02-09T00:00:00+00:00"><a href="http://localhost:4000/deep_learning_art_images/">February 09, 2019</a></time></span><span class="author vcard"><span class="fn"><a href="http://localhost:4000/about/" title="About Stephan Osterburg">Stephan Osterburg</a></span></span>
      
      <span class="entry-reading-time">
        <i class="fa fa-clock-o"></i>
        
Reading time ~3 minutes
      </span><!-- /.entry-reading-time -->
      
    </div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://localhost:4000/deep_learning_art_images/" rel="bookmark" title="Deep Learning and Art Images" itemprop="url">Deep Learning and Art Images</a></h1>
    
  </header>
  <div class="entry-content">
    <p>At <a href="https://www.kaggle.com">kaggle</a> we can find a dataset containing a collection of art images of google images, yandex images and from <a href="http://rusmuseumvrm.ru/collections/?lang=en">The Virtual Russian Museum</a>. The dataset has about 9000 images with five categories:</p>
<ol>
  <li>Drawings and Watercolors</li>
  <li>Paintings</li>
  <li>Sculpture</li>
  <li>Graphic Art</li>
  <li>Iconography (old Russian art)</li>
</ol>

<p>Because it is a classification problem, I wanted to use my newly learned knowledge in deep learning and convolutional neural networks (CNN). However, first things first, the downloaded zip file has two more zip files, which it turns out are somewhat similar in context but not quiet. So I decided to combine the two into one dataset.</p>

<h2 id="approach">Approach</h2>

<p>When I started to research how to tackle the issue for image classification, I found three possible options:</p>
<ul>
  <li><code class="highlighter-rouge">train_test_split</code>, we have to create the train and test data ourselves.</li>
  <li><code class="highlighter-rouge">flow_from_dataframe</code>, we need to create first the dataframe ourselves.</li>
  <li><code class="highlighter-rouge">flow_from_dictonary</code>, here we don’t need to do any extra work.</li>
</ul>

<p>I, as you may guessed it already, opted for the later.</p>

<h2 id="how-deep-is-too-deep">How deep is too deep?</h2>

<p>To answer that question, here we need to know one fact first. I am working on a MacBook Pro (2015) with a 2.2 GHz Intel Core i7 processor and 16GB of RAM.</p>

<p>So, how deep is too deep? How many layers can I run on my computer without feeling the pain? To find out I approached it very gingerly, step by step. Or shall I say layer by layer?</p>

<p>For the first try, I had only a few layers, which helped with the computation time. However, the result showed a test accuracy of less than 50%.</p>

<noscript><pre>400: Invalid request
</pre></noscript>
<script src="https://gist.github.com/8011d4e50fc6e22d9eb73b7763124003.js"> </script>

<p>To improve the test accuracy, I kept adding layers. I ended up adding several more layers, including <code class="highlighter-rouge">Dropout</code> layers to help to avoid over-fitting.</p>

<noscript><pre>400: Invalid request
</pre></noscript>
<script src="https://gist.github.com/57eff9c73a1f613847a2985a86c499fa.js"> </script>

<p>With the simple sequential model the computational time wasn’t too bad but with the  model, seen above, the time went up from several minutes per epochs to nearly 30 minutes per epochs.</p>

<h3 id="epochsbatch_size">epochs/batch_size</h3>

<p>An <code class="highlighter-rouge">epochs</code> is an iteration over the entire provided data; for example, if we have <code class="highlighter-rouge">epochs=25</code> we iterate 25 times over the data. The <code class="highlighter-rouge">batch_size</code> is the number of samples that will propagate through the network, by default 32. In our case where we have about 8000 images in the training set, we have 250 batches per epochs.</p>

<p>The question is, do we decrease the batch_size to 16 or increase the number to 64? If we decrease the number, we have 500 batches vs 125 batches if we increase the number. Large batch size result in faster processing time and vice versa. In regards to epochs, the model improves with more but only to a point. They start to plateau in accuracy as they converge.</p>

<h2 id="pre-trained-network">Pre-Trained Network</h2>

<p>To improve not only the accuracy but also the processing time (so I hoped), I decided to use a pre-trained network. There are a few we can choose from, for example <a href="https://keras.io/applications/#vgg16">VGG16</a>, <a href="https://keras.io/applications/#vgg19">VGG19</a>, <a href="https://keras.io/applications/#inceptionv3">InceptionV3</a>, and <a href="https://keras.io/applications/#resnet50">ResNet50</a> etcetera. I resorted to the <a href="https://arxiv.org/abs/1409.1556">VGG</a> model to keep it simple for the time being.</p>

<p><strong>VGG</strong> is a convolutional neural network model for image recognition proposed by the <strong>Visual Geometry Group in the University of Oxford</strong>, where <em>VGG16</em> refers to a VGG model with 16 weight layers, and <em>VGG19</em> refers to a VGG model with 19 weight layers.</p>

<p>Because we have an already trained model, all we have to do is add at least two more layers (<code class="highlighter-rouge">Flatten</code> and the output <code class="highlighter-rouge">Dense</code> layer) to test what the pre-trained network can do with our dataset. In my case I ended up adding six layers in total:</p>

<noscript><pre>400: Invalid request
</pre></noscript>
<script src="https://gist.github.com/2960515a509218482bbbff1a45add65c.js"> </script>

<p>Now, the accuracy improved to 96.68%, up &gt;5% from my previous model, but not the processing time.</p>

<h2 id="aws">AWS</h2>

<p>In the end, it took several hours to run the model. Moreover, it makes it even more painful if you forget to change your default setting on your MacBook and the computer goes into sleep mode, and nothing get processed at all.</p>

<p>To free up the computer I resorted to - welcome - <a href="https://www.paperspace.com">paperspace</a>.</p>

<p>Which allows me to prototype locally (prove of concept) and compute in the cloud.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Use where you can pre-trained networks and if you are using large data use cloud services.</p>


  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    
      <div class="entry-image-index">
        <a href="http://localhost:4000/an_introductio_to_diffusion_maps/" title="An Introduction to Diffusion Maps"><img src="http://localhost:4000/images/diffusion_map.png" alt="An Introduction to Diffusion Maps"></a>
      </div><!-- /.entry-image -->
    
    <div class="entry-meta">
      <span class="entry-date date published updated"><time datetime="2019-01-21T00:00:00+00:00"><a href="http://localhost:4000/an_introductio_to_diffusion_maps/">January 21, 2019</a></time></span><span class="author vcard"><span class="fn"><a href="http://localhost:4000/about/" title="About Stephan Osterburg">Stephan Osterburg</a></span></span>
      
      <span class="entry-reading-time">
        <i class="fa fa-clock-o"></i>
        
Reading time ~2 minutes
      </span><!-- /.entry-reading-time -->
      
    </div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://localhost:4000/an_introductio_to_diffusion_maps/" rel="bookmark" title="An Introduction to Diffusion Maps" itemprop="url">An Introduction to Diffusion Maps</a></h1>
    
  </header>
  <div class="entry-content">
    <p>The <a href="https://inside.mines.edu/~whereman/talks/delaPorte-Herbst-Hereman-vanderWalt-DiffusionMaps-PRASA2008.pdf">paper</a> published by J. de la Porte<sup id="fnref:4"><a href="#fn:4" class="footnote">1</a></sup>, B. M. Herbst<sup id="fnref:4:1"><a href="#fn:4" class="footnote">1</a></sup>, W. Hereman<sup id="fnref:5"><a href="#fn:5" class="footnote">2</a></sup> and S. J. van der Walt<sup id="fnref:4:2"><a href="#fn:4" class="footnote">1</a></sup> in November 2008 describes the mathematical technique for dealing with reducing dimensionality in data. The authors give an overview of three well-known dimensionality reduction techniques and introduce diffusion maps and its functionality, process and how it compares with the other methods.</p>

<h3 id="dimensionality-reduction">Dimensionality Reduction</h3>

<p>We are limited in visualising information beyond the third dimension. For simplicity reasons, imagine a black and white image (grey scale) of 100 by 100 pixels, where each pixel represents a variable. The dimensionality would add up to be 10,000. A <strong>human</strong> wouldn’t have any issues with reading the following digits in the picture below, but a <strong>computer</strong> sees them as a data point in an <em>nm</em>-dimensional coordinate space.</p>

<figure>
    
    <a href="/images/Two-images-of-the-same-digit-at-different-rotation-angles.png"><img src="/images/Two-images-of-the-same-digit-at-different-rotation-angles.png" alt="" /></a>
    
    <figcaption></figcaption>
</figure>

<p>Dimensionality Reduction is about converting data of very high dimensionality into data of much lower dimensionality such that each of the lower dimensions conveys much more information.</p>

<p>Here we can use the analogy of zipping text files, compressing large text data into smaller equivalent data. With the minor but significant difference that in dimensionality reduction we will lose some information. The goal here is to not lose too many of the essential features in the data.</p>

<h3 id="popular-techniques">Popular Techniques</h3>

<p>The most popular reduction methods are</p>

<ul>
  <li>Principle Component Analysis (PCA)<sup id="fnref:6"><a href="#fn:6" class="footnote">3</a></sup>, a linear dimensionality reduction technique using Singular Value Decomposition of the data to project it to a lower dimensional space.<sup id="fnref:7"><a href="#fn:7" class="footnote">4</a></sup></li>
  <li>Multi-dimensional Scaling (MDS)<sup id="fnref:8"><a href="#fn:8" class="footnote">5</a></sup> seeks a low-dimensional representation of the data in which the distances respect well the distances in the original high-dimensional space.<sup id="fnref:9"><a href="#fn:9" class="footnote">6</a></sup></li>
  <li>Isometric Feature Map (Isomap)<sup id="fnref:10"><a href="#fn:10" class="footnote">7</a></sup>, a non-linear dimensionality reduction through Isometric Mapping.<sup id="fnref:11"><a href="#fn:11" class="footnote">8</a></sup></li>
</ul>

<h3 id="diffusion-maps">Diffusion Maps</h3>

<p>Diffusion maps are a non-linear technique. In which we analyse first the connectivity of the data. Similar to taking a random walk on our data, where we jump between data points in feature space (see image below). The goal is to reveal the geometric structure of our data at different scales, defining a “time-dependent” diffusion metrics. Here <em>time</em> seems to be interchangeable with <em>scale</em>.</p>

<figure>
    
    <a href="/images/A-random-walk-on-a-data-set-Each-jump-has-a-probability-associated-with-it-The-green.png"><img src="/images/A-random-walk-on-a-data-set-Each-jump-has-a-probability-associated-with-it-The-green.png" alt="" /></a>
    
    <figcaption></figcaption>
</figure>

<p>By measuring the distances between data points, we can define neighbourhoods. In order to achieve it, we define a threshold value in the way of a function (kernel) to obtain the similarity within a neighboorhood.</p>

<h5 id="footnotes">Footnotes</h5>

<!-- An example of a Gist embed below. -->
<!-- <noscript><pre>404: Not Found
</pre></noscript><script src="https://gist.github.com/mmistakes/6589546.js"> </script> -->
<div class="footnotes">
  <ol>
    <li id="fn:4">
      <p>Applied Mathematics Division, Department of Mathematical Sciences, University of Stellenbosch, South Africa <a href="#fnref:4" class="reversefootnote">&#8617;</a> <a href="#fnref:4:1" class="reversefootnote">&#8617;<sup>2</sup></a> <a href="#fnref:4:2" class="reversefootnote">&#8617;<sup>3</sup></a></p>
    </li>
    <li id="fn:5">
      <p>Colorado School of Mines, United States of America <a href="#fnref:5" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:6">
      <p>I.T. Jolliffe. Principal component analysis. SpringerVerlag New York, 1986. <a href="#fnref:6" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:7">
      <p><a href="https://scikit-learn.org/stable/modules/decomposition.html#pca">https://scikit-learn.org/stable/modules/decomposition.html#pca</a> <a href="#fnref:7" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:8">
      <p>W.S. Torgerson. Multidimensional scaling: I. Theory and method. Psychometrika, 17(4):401–419, 1952. <a href="#fnref:8" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:9">
      <p><a href="https://scikit-learn.org/stable/modules/manifold.html#multidimensional-scaling">https://scikit-learn.org/stable/modules/manifold.html#multidimensional-scaling</a> <a href="#fnref:9" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:10">
      <p>Joshua B. Tenenbaum, Vin de Silva, and John C. Langford. A Global Geometric Framework for Nonlinear Dimensionality Reduction. Science, 290(5500):2319–2323, 2000. <a href="#fnref:10" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:11">
      <p><a href="https://scikit-learn.org/stable/modules/manifold.html#isomap">https://scikit-learn.org/stable/modules/manifold.html#isomap</a> <a href="#fnref:11" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>

  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    
      <div class="entry-image-index">
        <a href="http://localhost:4000/forest/" title="Forest Cover Type"><img src="http://localhost:4000/images/dataset-cover.jpeg" alt="Forest Cover Type"></a>
      </div><!-- /.entry-image -->
    
    <div class="entry-meta">
      <span class="entry-date date published updated"><time datetime="2019-01-14T00:00:00+00:00"><a href="http://localhost:4000/forest/">January 14, 2019</a></time></span><span class="author vcard"><span class="fn"><a href="http://localhost:4000/about/" title="About Stephan Osterburg">Stephan Osterburg</a></span></span>
      
      <span class="entry-reading-time">
        <i class="fa fa-clock-o"></i>
        
Reading time ~6 minutes
      </span><!-- /.entry-reading-time -->
      
    </div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://localhost:4000/forest/" rel="bookmark" title="Forest Cover Type" itemprop="url">Forest Cover Type</a></h1>
    
  </header>
  <div class="entry-content">
    <h2 id="or-who-cannot-see-the-wood-for-the-trees">Or who cannot see the wood for the trees?</h2>

<p>Researchers at the Department of Forest Sciences at Colorado State University collected over half a million measurements from tree observations from four areas of the Roosevelt National Forest in Colorado. All observations are cartographic variables (no remote sensing) from 30-meter x 30-meter sections of forest.</p>

<p>The resulting dataset includes information on tree type, shadow coverage, distance to nearby landmarks (roads etcetera), soil type, and local topography. In total there are 55 columns/features.</p>

<h3 id="problem">Problem</h3>

<p>Can we build a model that predicts what types of trees grow in an area based on the surrounding characteristics? Like elevation, slope, distance, soil type etcetera.</p>

<h3 id="dataset">Dataset</h3>

<p>The dataset has 55 columns in total where <code class="highlighter-rouge">Wilderness_Area</code> consists of 4 dummy variables and <code class="highlighter-rouge">Soil_Tpye</code> consists of 40 dummy variables.</p>

<p>Continuous Data</p>
<ul>
  <li><code class="highlighter-rouge">Elevation</code> (in meters)</li>
  <li><code class="highlighter-rouge">Aspect</code> (in degrees azimuth<sup id="fnref:2"><a href="#fn:2" class="footnote">1</a></sup>)</li>
  <li><code class="highlighter-rouge">Slope</code> (in degrees)</li>
  <li><code class="highlighter-rouge">Horizontal_Distance_To_Hydrology</code> (Horizontal distance to nearest surface water features in meters)</li>
  <li><code class="highlighter-rouge">Horizontal_Distance_To_Roadways</code> (Horizontal distance to nearest roadway in meters)</li>
  <li><code class="highlighter-rouge">Horizontal_Distance_To_Fire_Points</code> (Horizontal distance to nearest wildfire ignition points in meters)</li>
  <li><code class="highlighter-rouge">Vertical_Distance_To_Hydrology</code> (Vertical distance to nearest surface water features in meters)</li>
  <li><code class="highlighter-rouge">Hillshade_9am</code> (Hill shade index at 9am, summer solstice. Value out of 255)</li>
  <li><code class="highlighter-rouge">Hillshade_Noon</code> (Hill shade index at noon, summer solstice. Value out of 255)</li>
  <li><code class="highlighter-rouge">Hillshade_3pm</code> (Hill shade index at 3pm, summer solstice. Value out of 255)</li>
</ul>

<p>Categorical Data</p>
<ul>
  <li><code class="highlighter-rouge">Wilderness Area</code> (4 dummy variable binary columns, 0 = absence or 1 = presence)</li>
  <li><code class="highlighter-rouge">Soil Type</code> (40 dummy variable binary columns, 0 = absence or 1 = presence)</li>
</ul>

<p>The target variable <code class="highlighter-rouge">Cover_Type</code> is defined as an integer value between <code class="highlighter-rouge">1</code> and <code class="highlighter-rouge">7</code>, with the following key:</p>

<ol>
  <li>Spruce/Fir.</li>
  <li>Lodgepole Pine.</li>
  <li>Ponderosa Pine.</li>
  <li>Cottonwood/Willow</li>
  <li>Aspen</li>
  <li>Douglas-fir</li>
  <li>Krummholz</li>
</ol>

<h3 id="approach">Approach</h3>

<h4 id="eda">EDA</h4>

<p>The first step with every dataset is to do an <code class="highlighter-rouge">Exploratory Data Analysis</code> (EDA). What kind of data do we have? Text or numerical (continues or categorical)? Do we have missing data, or just merely wrong data — values which do not make any sense in the given context, i.e., 9999.</p>

<p>Because of the sheer number of soil types and wilderness areas I had a look at them first. Luckily, the researchers here were efficient in the sense that they document everything meticulously. Everything we need to know with a detailed description can be found online<sup id="fnref:3"><a href="#fn:3" class="footnote">2</a></sup>.</p>

<p>From that document, we can find out that</p>
<blockquote>
  <p>This study area includes four wilderness areas located in the
   Roosevelt National Forest of northern Colorado.  These areas
   represent forests with minimal human-caused disturbances,
   so that existing forest cover types are more a result of 
   ecological processes rather than forest management practices.</p>
</blockquote>

<p>Not that this is of any importance for our data exploration or to make a prediction, but I found it interesting to read. Because it makes you think about what kind of implications your result can potentially have.</p>

<p>However, what we want to find out is how the data is distributed by <code class="highlighter-rouge">Cover_Type</code>. Here I created a <code class="highlighter-rouge">seaborn.countplot</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tmpList</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">soil_df</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
    <span class="n">tmpList</span> <span class="o">+=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">c</span><span class="p">)]</span> <span class="o">*</span> <span class="n">soil_df</span><span class="p">[</span><span class="n">c</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>

<span class="n">se</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">tmpList</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s">'Soil_Types'</span><span class="p">]</span> <span class="o">=</span> <span class="n">se</span><span class="o">.</span><span class="n">values</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">countplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s">'Soil_Types'</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s">'Cover_Type'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Number of Observation by Cover Type'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">();</span>
</code></pre></div></div>

<figure>
    
    <a href="/images/SoilType.png"><img src="/images/SoilType.png" alt="" /></a>
    
    <figcaption></figcaption>
</figure>

<p>Looking more closely at the data we can find that the top 5 soil types count for more than 50% of the measurements in the collected data.</p>

<p>Unlike categorical data, the continues data is even more interesting to investigate. The questions we want to find answers for are</p>

<ul>
  <li>Do we need to scale/normalise the continuous data?</li>
  <li>What about skewness/kurtosis?</li>
  <li>Does it matter if the data is in meter, degree or index?</li>
</ul>

<figure>
    
    <a href="/images/ContinuesData.png"><img src="/images/ContinuesData.png" alt="" /></a>
    
    <figcaption></figcaption>
</figure>

<h3 id="feature-selection">Feature Selection</h3>

<p>There are several ways to explore what features we need to keep around to make our prediction. The labour intensive workflow and the much quicker workflow - what I would like to call the “blindfolded method”. For the purpose of gaining more inside, I choose to do both.</p>

<p>In the first workflow, I used several classifiers from the <code class="highlighter-rouge">sklearn.ensemble</code> Library. These are, <code class="highlighter-rouge">AdaBoostClassifier</code>,  <code class="highlighter-rouge">RandomForestClassifier</code>, <code class="highlighter-rouge">GradientBoostingClassifier</code> and <code class="highlighter-rouge">ExtraTreeClassifier</code>. All these classifiers have one thing in common, which is the attribute <code class="highlighter-rouge">feature_importances_</code>, which returns the feature importance (the higher the value, the more important the feature).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create an empty dataframe to hold our findings for feature_importances_
</span><span class="n">ranking_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>

<span class="n">RFC_model</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">RFC_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">importances</span> <span class="o">=</span> <span class="n">RFC_model</span><span class="o">.</span><span class="n">feature_importances_</span>
<span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">importances</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># Get feature name
</span><span class="n">rfc_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">indices</span><span class="p">[</span><span class="n">f</span><span class="p">]]</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>
<span class="n">ranking_df</span><span class="p">[</span><span class="s">'RFC'</span><span class="p">]</span> <span class="o">=</span> <span class="n">rfc_list</span>

<span class="c1"># Get feature importance
</span><span class="n">rfci_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">importances</span><span class="p">[</span><span class="n">indices</span><span class="p">[</span><span class="n">f</span><span class="p">]]</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>
<span class="n">ranking_df</span><span class="p">[</span><span class="s">'RFC importance'</span><span class="p">]</span> <span class="o">=</span> <span class="n">rfci_list</span>
</code></pre></div></div>

<p>The result was a pandas data frame with all features from the dataset in order of importance, which allows us to pick the best features. Interestingly, <code class="highlighter-rouge">RandomForestClassifier</code> and <code class="highlighter-rouge">ExtraTreeClassifier</code> had the most similar results. Other findings from that list were,</p>

<ul>
  <li><code class="highlighter-rouge">Gradian Boosting</code> shows similar names just in a different order compared to <code class="highlighter-rouge">Random Forest</code> and <code class="highlighter-rouge">Extra Tree</code>.</li>
  <li><code class="highlighter-rouge">AdaBoost</code> on the other hand shows an exciting and unique result. The top 8 features alone are enough to make a good class prediction. Compared to all the other classifiers, here we have <code class="highlighter-rouge">Wilderness_Area4</code> on above <code class="highlighter-rouge">Elevation</code> in the list.</li>
  <li><code class="highlighter-rouge">Elevation</code> dominates in all classifiers with a range of <code class="highlighter-rouge">18-25%</code>, up to <code class="highlighter-rouge">65%</code> in <code class="highlighter-rouge">GBC</code>.</li>
  <li><code class="highlighter-rouge">Hillshade</code> features are seen in the top 20 in 3 out of 4 classifiers. <code class="highlighter-rouge">Random Forest</code> and <code class="highlighter-rouge">Extra Tree Classifier</code> show <code class="highlighter-rouge">Hillshade</code> features having similar ranges.</li>
  <li><code class="highlighter-rouge">Horizontal_Distance_To_Hydrology</code> and <code class="highlighter-rouge">Vertical_Distance_To_Hydrology</code> are in all classifiers in the top 10.</li>
  <li><code class="highlighter-rouge">Horizontal_Distance_To_Roadways</code> and <code class="highlighter-rouge">Horizontal_Distance_To_Fire_Points</code> are represented at the top 3 out of 4 classifiers.</li>
  <li><code class="highlighter-rouge">Aspect</code> and <code class="highlighter-rouge">Slope</code> also show up in the top 20 across all classifiers, with the exception of Gradian Boosting Slope which isn’t in the top 20.</li>
  <li>In regards to <code class="highlighter-rouge">Soil_Type</code>, it is hard to find some commonality. Here I choose to select: <code class="highlighter-rouge">Soil_Type2</code>, <code class="highlighter-rouge">Soil_Type4</code>, <code class="highlighter-rouge">Soil_Type10</code>, <code class="highlighter-rouge">Soil_Type22</code>, <code class="highlighter-rouge">Soil_Type23</code> and <code class="highlighter-rouge">Soil_Type39</code>.</li>
</ul>

<p>The question I asked myself at this point is, was the time wisely spend? After all, it took almost 20 minutes to calculate. One advantage this approach has is, we know our selected features.</p>

<p>The other approach we can choose is using <code class="highlighter-rouge">PCA</code> (Principal Component Analysis) from sklearn.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">'Cover_Type'</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s">'Cover_Type'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">X_pca</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
</code></pre></div></div>

<p>The big advantage to this is the short processing time. However, the drawback, we are losing any knowledge regarding our features. Is that important? Possibly.</p>

<p>As you can see in the code snippet above, I opted to scale the features. Researching that topic, I found that there is a multitude of approaches, some use a combination of <code class="highlighter-rouge">StandardScaler</code>, <code class="highlighter-rouge">MinMaxScaler</code> and <code class="highlighter-rouge">Normalizer</code> method and others just picked one of the methods for scaling, centring, normalisation, binarisation and imputation the <code class="highlighter-rouge">sklearn.preprocessing</code> module. Some just scale only the continuous features and not the categorical, etcetera. The list of possibilities goes on. For now, I just used the most straightforward way.</p>

<h3 id="evaluate-model">Evaluate Model</h3>

<p>By hand selecting our features, we get an <code class="highlighter-rouge">accuracy of 92.23%</code> and an <code class="highlighter-rouge">f1_score of 86.74%</code> using cross validation. In contrast, <code class="highlighter-rouge">PCA</code> has an <code class="highlighter-rouge">accuracy of 90.72%</code> and an <code class="highlighter-rouge">f1_score of 84.44%</code>. Is that a big enough difference to justify the time spent to find our features? Maybe.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_pca</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="s">'distance'</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">accuracy</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">scoring</span> <span class="o">=</span> <span class="s">'accuracy'</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">f1_score</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">scoring</span> <span class="o">=</span> <span class="s">'f1_macro'</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># predict
</span><span class="n">predict</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># calculating accuracy
</span><span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predict</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">'KNeighbors Classifier model'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Accuracy: {:.2f}</span><span class="si">%</span><span class="s">'</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">accuracy</span> <span class="o">*</span> <span class="mi">100</span><span class="p">))</span>

<span class="n">knn_classification_report</span> <span class="o">=</span> <span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predict</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">knn_classification_report</span><span class="p">)</span>
</code></pre></div></div>

<figure>
    
    <a href="/images/CrossValidation.png"><img src="/images/CrossValidation.png" alt="" /></a>
    
    <figcaption></figcaption>
</figure>

<p>To test how accurate we can predict the <code class="highlighter-rouge">Cover_Type</code> I used the <code class="highlighter-rouge">KNeighborsClassifier</code> from sklearn, and we got an accuracy of <code class="highlighter-rouge">91.02%</code>. That is great.</p>

<h2 id="conclusion">Conclusion</h2>

<p><strong>Yes</strong>, we can build a model that predicts what types of trees grow in an area based on the surrounding characteristics.</p>

<h2 id="final-thought">Final Thought</h2>

<p>We can use sklearn’s <code class="highlighter-rouge">GridSearchCV</code> to find our features (exhaustive search over specified parameter values for an estimator). Moreover, we should make use of sklearn’s <code class="highlighter-rouge">Pipeline</code> functionality to write clean and manageable code.</p>

<p>One last question, could we use clustering to make our prediction?</p>

<h5 id="footnotes">Footnotes</h5>

<noscript><pre>404: Not Found
</pre></noscript>
<script src="https://gist.github.com/mmistakes/6589546.js"> </script>

<div class="footnotes">
  <ol>
    <li id="fn:2">
      <p><a href="https://en.wikipedia.org/wiki/Azimuth">https://en.wikipedia.org/wiki/Azimuth</a> <a href="#fnref:2" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p><a href="https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.info">https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.info</a> <a href="#fnref:3" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>

  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    
      <div class="entry-image-index">
        <a href="http://localhost:4000/mapping_data_with_folium/" title="Mapping Data with folium"><img src="http://localhost:4000/images/9background.jpg" alt="Mapping Data with folium"></a>
      </div><!-- /.entry-image -->
    
    <div class="entry-meta">
      <span class="entry-date date published updated"><time datetime="2018-12-07T00:00:00+00:00"><a href="http://localhost:4000/mapping_data_with_folium/">December 07, 2018</a></time></span><span class="author vcard"><span class="fn"><a href="http://localhost:4000/about/" title="About Stephan Osterburg">Stephan Osterburg</a></span></span>
      
      <span class="entry-reading-time">
        <i class="fa fa-clock-o"></i>
        
Reading time ~3 minutes
      </span><!-- /.entry-reading-time -->
      
    </div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://localhost:4000/mapping_data_with_folium/" rel="bookmark" title="Mapping Data with folium" itemprop="url">Mapping Data with folium</a></h1>
    
  </header>
  <div class="entry-content">
    <p>For my first Data Science project at Flatiron, we got the King County Housing dataset from Kaggle to work with.</p>

<p>I first wanted to know where King County is. Is the county in a rural area, suburban area or just a big city like New York or a combination of some or all of them. To get a basic understanding and feel for what is the data represents. After going through the scrubbing part and dealing with the not so clean data handed, and having a better understanding of what is in the dataset, which leads me to the following questions:</p>

<ul>
  <li>Is location of a house by zip code (neighbourhood) an indicator for the house price?</li>
  <li>Do zip codes (neighbourhoods) with the higher housing density have an effect on selling price?</li>
</ul>

<p>Also, what visualisation technique should or can I use to present it best? My choice was to use a map to visualise the data. For that, I picked the folium package.</p>

<p>The dataset has (not all) the zip codes for King County. To visualise the zip codes, we need the geojson data. Which we luckily can find on the King County website.</p>

<p>First, we want to get all zip codes in the dataset into a separate dataset.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Set zipcode type to string (folium)
</span><span class="n">dataset</span><span class="p">[</span><span class="s">'zipcode'</span><span class="p">]</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s">'zipcode'</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s">'str'</span><span class="p">)</span>
<span class="err">​</span>
<span class="c1"># get the mean value across all data points
</span><span class="n">zipcode_data</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s">'zipcode'</span><span class="p">)</span><span class="o">.</span><span class="n">aggregate</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">)</span>
<span class="n">zipcode_data</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">inplace</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p>For this example, we want to display the housing density. So we need to sum all zip codes up and add it to our zipcode_data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># count number of houses grouped by zipcode
</span><span class="n">dataset</span><span class="p">[</span><span class="s">'count'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">temp</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s">'zipcode'</span><span class="p">)</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span>
<span class="n">temp</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">inplace</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">temp</span> <span class="o">=</span> <span class="n">temp</span><span class="p">[[</span><span class="s">'zipcode'</span><span class="p">,</span> <span class="s">'count'</span><span class="p">]]</span>
<span class="n">zipcode_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">zipcode_data</span><span class="p">,</span> <span class="n">temp</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="s">'zipcode'</span><span class="p">)</span>
<span class="err">​</span>
<span class="c1"># drop count from org dataset
</span><span class="n">dataset</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s">'count'</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p>Before we move on to display “all” zip codes in our dataset, we want to make sure that the geodata has only the zip codes found in our dataset. (Note, this code is not my brainchild).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Get geo data file path
</span><span class="n">geo_data_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s">'data'</span><span class="p">,</span> <span class="s">'king_county_wa_zipcode_area.geojson'</span><span class="p">)</span>
<span class="c1"># load GeoJSON
</span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">geo_data_file</span><span class="p">,</span> <span class="s">'r'</span><span class="p">)</span> <span class="k">as</span> <span class="n">jsonFile</span><span class="p">:</span>
    <span class="n">geo_data</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">jsonFile</span><span class="p">)</span>
    
<span class="n">tmp</span> <span class="o">=</span> <span class="n">geo_data</span>
<span class="err">​</span>
<span class="c1"># remove ZIP codes not in geo data
</span><span class="n">geozips</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tmp</span><span class="p">[</span><span class="s">'features'</span><span class="p">])):</span>
    <span class="k">if</span> <span class="n">tmp</span><span class="p">[</span><span class="s">'features'</span><span class="p">][</span><span class="n">i</span><span class="p">][</span><span class="s">'properties'</span><span class="p">][</span><span class="s">'ZIPCODE'</span><span class="p">]</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">zipcode_data</span><span class="p">[</span><span class="s">'zipcode'</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">()):</span>
        <span class="n">geozips</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tmp</span><span class="p">[</span><span class="s">'features'</span><span class="p">][</span><span class="n">i</span><span class="p">])</span>
        
<span class="c1"># creating new JSON object
</span><span class="n">new_json</span> <span class="o">=</span> <span class="nb">dict</span><span class="o">.</span><span class="n">fromkeys</span><span class="p">([</span><span class="s">'type'</span><span class="p">,</span><span class="s">'features'</span><span class="p">])</span>
<span class="n">new_json</span><span class="p">[</span><span class="s">'type'</span><span class="p">]</span> <span class="o">=</span> <span class="s">'FeatureCollection'</span>
<span class="n">new_json</span><span class="p">[</span><span class="s">'features'</span><span class="p">]</span> <span class="o">=</span> <span class="n">geozips</span>
<span class="err">​</span>
<span class="c1"># save uodated JSON object
</span><span class="nb">open</span><span class="p">(</span><span class="s">"cleaned_geodata.json"</span><span class="p">,</span> <span class="s">"w"</span><span class="p">)</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">new_json</span><span class="p">,</span> <span class="n">sort_keys</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">separators</span><span class="o">=</span><span class="p">(</span><span class="s">','</span><span class="p">,</span> <span class="s">': '</span><span class="p">)))</span>
</code></pre></div></div>

<p>Finally, we have our function to display our map and housing density. I wrote it as a function I that I could easily visualise other features like house prices.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">map_feature_by_zipcode</span><span class="p">(</span><span class="n">zipcode_data</span><span class="p">,</span> <span class="n">col</span><span class="p">):</span>
    <span class="s">""" Generates a folium map of Seattle 
    :param zipcode_data: zipcode dataset 
    :param col: feature to display 
    :return: m 
    """</span>
<span class="err">​</span>
    <span class="c1"># read updated geo data
</span>    <span class="n">king_geo</span> <span class="o">=</span> <span class="s">"cleaned_geodata.json"</span>
<span class="err">​</span>
    <span class="c1"># Initialize Folium Map with Seattle latitude and longitude
</span>    <span class="n">m</span> <span class="o">=</span> <span class="n">folium</span><span class="o">.</span><span class="n">Map</span><span class="p">(</span><span class="n">location</span><span class="o">=</span><span class="p">[</span><span class="mf">47.35</span><span class="p">,</span> <span class="o">-</span><span class="mf">121.9</span><span class="p">],</span> <span class="n">zoom_start</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span>
                   <span class="n">detect_retina</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">control_scale</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="err">​</span>
    <span class="c1"># Create choropleth map
</span>    <span class="n">m</span><span class="o">.</span><span class="n">choropleth</span><span class="p">(</span>
        <span class="n">geo_data</span><span class="o">=</span><span class="n">king_geo</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="s">'choropleth'</span><span class="p">,</span>
        <span class="n">data</span><span class="o">=</span><span class="n">zipcode_data</span><span class="p">,</span>
        <span class="c1"># col: feature of interest
</span>        <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s">'zipcode'</span><span class="p">,</span> <span class="n">col</span><span class="p">],</span>
        <span class="n">key_on</span><span class="o">=</span><span class="s">'feature.properties.ZIPCODE'</span><span class="p">,</span>
        <span class="n">fill_color</span><span class="o">=</span><span class="s">'OrRd'</span><span class="p">,</span>
        <span class="n">fill_opacity</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
        <span class="n">line_opacity</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
        <span class="n">legend_name</span><span class="o">=</span><span class="s">'house '</span> <span class="o">+</span> <span class="n">col</span>
    <span class="p">)</span>
<span class="err">​</span>
    <span class="n">folium</span><span class="o">.</span><span class="n">LayerControl</span><span class="p">()</span><span class="o">.</span><span class="n">add_to</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
<span class="err">​</span>
    <span class="c1"># Save map based on feature of interest
</span>    <span class="n">m</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">col</span> <span class="o">+</span> <span class="s">'.html'</span><span class="p">)</span>
<span class="err">​</span>
    <span class="k">return</span> <span class="n">m</span>
</code></pre></div></div>

<p>…and our function call.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">f</span><span class="o">.</span><span class="n">map_feature_by_zipcode</span><span class="p">(</span><span class="n">zipcode_data</span><span class="p">,</span> <span class="s">'count'</span><span class="p">)</span>
</code></pre></div></div>

<figure>
    
    <a href="/images/seattle_density_map.png"><img src="/images/seattle_density_map.png" alt="" /></a>
    
    <figcaption></figcaption>
</figure>

<p>The main strength I can see in our case is that we can answer our question regarding housing density by zip code.</p>

<p>The weakness here, unfortunately, lies in our given dataset itself. Because we only have a subset of about 22k houses represented. Another weakness I can see is. House prices are not so much related to zip codes, instead to neighbourhoods. If we go and look at Zillow, we can search by neighbourhood. Moreover, Zillow offers a geojson file for that.</p>

<p>However, I guess I get myself over my head here.</p>


  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    
      <div class="entry-image-index">
        <a href="http://localhost:4000/why/" title="“Why did you decide to learn data science?”"><img src="http://localhost:4000/images/Tokyo.png" alt="“Why did you decide to learn data science?”"></a>
      </div><!-- /.entry-image -->
    
    <div class="entry-meta">
      <span class="entry-date date published updated"><time datetime="2018-12-05T00:00:00+00:00"><a href="http://localhost:4000/why/">December 05, 2018</a></time></span><span class="author vcard"><span class="fn"><a href="http://localhost:4000/about/" title="About Stephan Osterburg">Stephan Osterburg</a></span></span>
      
      <span class="entry-reading-time">
        <i class="fa fa-clock-o"></i>
        
Reading time ~1 minute
      </span><!-- /.entry-reading-time -->
      
    </div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://localhost:4000/why/" rel="bookmark" title="“Why did you decide to learn data science?”" itemprop="url">“Why did you decide to learn data science?”</a></h1>
    
  </header>
  <div class="entry-content">
    <p>I don’t remember when I came across “The Visual Display of Quantitative Information” by Edward R.Tufte<sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup>, but I remember that I was fascinated by it and read it in one swoop. Moreover, I just wanted to do cool visualisations. However, at that time there was no need for it.</p>

<p>Sometime in the early nineties (I believe), I saw a short film by a Japanese computer graphics company visualising all data available at that time about Tokyo. I was just fascinated by it and just wanted to do that. Still, at that time there was no real need for it. At least not in Germany.</p>

<p>Fast forward, another Edward Tufte book and much more cool things over the time, I found myself at a crossroad. Do I take the blue pill or the red pill? Because I can’t remember which tablet does what, I decided to take some time off to figure out what I would love to do next.</p>

<p>So here I am, rediscovering my fascination for data and visualisation. Looking forward to using my experience in computer graphics and combine it with the newly learned knowledge.</p>

<noscript><pre>404: Not Found
</pre></noscript>
<script src="https://gist.github.com/mmistakes/6589546.js"> </script>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p><a href="https://www.edwardtufte.com/tufte/books_vdqi">https://www.edwardtufte.com/tufte/books_vdqi</a> <a href="#fnref:1" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>

  </div><!-- /.entry-content -->
</article><!-- /.hentry -->



</div><!-- /#main -->

<div class="footer-wrapper">
  <footer role="contentinfo">
    <span>&copy; 2019 Stephan Osterburg. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> using the <a href="https://mademistakes.com/work/hpstr-jekyll-theme/" rel="nofollow">HPSTR Theme</a>.</span>
  </footer>
</div><!-- /.footer-wrapper -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="http://localhost:4000/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="http://localhost:4000/assets/js/scripts.min.js"></script>



          

</body>
</html>